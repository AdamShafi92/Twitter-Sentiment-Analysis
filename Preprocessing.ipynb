{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#import natural language processing toolkit libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords as stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import emoji\n",
    "from string import punctuation\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>user</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>7683</td>\n",
       "      <td>_DPone_</td>\n",
       "      <td>Fri May 08 17:40:59 +0000 2020</td>\n",
       "      <td>#Markets #Stocks $SPX $SPY $ES_F $IXIC $DJI $QQQ</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'text': 'Markets', 'indices': [0, 8]}, {'tex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  Unnamed: 1     user                            date  \\\n",
       "0      2        7683  _DPone_  Fri May 08 17:40:59 +0000 2020   \n",
       "\n",
       "                                               text  favorite_count  \\\n",
       "0  #Markets #Stocks $SPX $SPY $ES_F $IXIC $DJI $QQQ               0   \n",
       "\n",
       "                                            hashtags  \n",
       "0  [{'text': 'Markets', 'indices': [0, 8]}, {'tex...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Test Data.csv')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 1','user','date'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove blank tweets\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "#filter to only cashtags or money\n",
    "df = df[df.text.str.contains('\\$')]\n",
    "\n",
    "#clean up ampersands\n",
    "df['text'] = df['text'].str.replace('&amp;', '&', case=False)\n",
    "\n",
    "#Remove retweets\n",
    "df = df[~df.text.str.contains('^RT', regex=True)]   # ~ (tilde) is INVERT - does the opposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Emojis\n",
    "def extract_emojis(abc):\n",
    "    return ''.join(c for c in abc if c in emoji.UNICODE_EMOJI)\n",
    "\n",
    "df['emoji']=df['text'].apply(extract_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Cashtags\n",
    "a=[]\n",
    "cashtagsrx = re.compile(r'\\$[A-Z]{2,}')\n",
    "for i in df['text']:\n",
    "    a.append(cashtagsrx.findall(str(i)))\n",
    "df['Cashtags'] = a\n",
    "\n",
    "#remove rows without cashtags\n",
    "df = df.dropna(subset=['Cashtags']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Hashtags and add to df\n",
    "\n",
    "hashtag_list = []\n",
    "for row in df['hashtags']:\n",
    "    row = literal_eval(row) #issue with lists not being read as lists\n",
    "    x = [x['text'] for x in row if 'text' in x] \n",
    "    hashtag_list.append(x)\n",
    "    \n",
    "df['hashtag_list'] = hashtag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to apply regex to column\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in range(len(r)):\n",
    "        r[i] = '\\\\' + r[i]\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove text that isn't useful for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cashtags \n",
    "df['clean_tweet'] = np.vectorize(remove_pattern)(df['text'], r'\\$[A-Z]{2,}') \n",
    "\n",
    "#urls\n",
    "df['clean_tweet'] = df['clean_tweet'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True) \n",
    "\n",
    "#lowercase tweet\n",
    "df['clean_tweet'] = df['clean_tweet'].map(lambda x: x.lower()) \n",
    "\n",
    "#remove @name\n",
    "df['clean_tweet'] = df['clean_tweet'].replace('@[^\\s]+', '', regex=True)\n",
    "\n",
    "#remove punctuation\n",
    "df['clean_tweet'] = df['clean_tweet'].str.replace(\"[^a-zA-Z]\", \" \") \n",
    "\n",
    "#remove short words\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tokens - list of individual words rather than string\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = stopwords.words('english') #stopwords are words that aren't used in classification such as and, then, for\n",
    "\n",
    "#function to remove these from a tokenised list\n",
    "def remove_stopwords(tokenised_list):\n",
    "    text = [word for word in tokenised_list if word not in stopword]\n",
    "    return text\n",
    "\n",
    "#apply function\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove blank tweets after preprocessing (eg tweet is only stopwords)\n",
    "df = df.dropna(subset=['clean_tweet']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag the words for lemmatisation\n",
    "#this adds a tag depending on the type of word - eg 'learn = verb' \n",
    "#this is because different word types need different changes to get back to the root word\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x: pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to lemmatize sentence - removing 'ing' 'ly' etc from the end of the word\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in tokens:\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x: lemmatize_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['text','favorite_count','hashtags'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = df['Label'].replace(3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>emoji</th>\n",
       "      <th>Cashtags</th>\n",
       "      <th>hashtag_list</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>[$SPX, $SPY, $ES, $IXIC, $DJI, $QQQ]</td>\n",
       "      <td>[Markets, Stocks]</td>\n",
       "      <td>[market, stock]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ðŸ‡ºðŸ‡¸ðŸ‡ºðŸ‡¸</td>\n",
       "      <td>[$SPX, $SPY, $MSFT]</td>\n",
       "      <td>[stocks]</td>\n",
       "      <td>[earnings, growth, rate, among, large, stock, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>ðŸ‡ºðŸ‡¸ðŸ‡ºðŸ‡¸</td>\n",
       "      <td>[$SPX, $SPY, $CCL, $WES]</td>\n",
       "      <td>[stocks]</td>\n",
       "      <td>[bad, perform, stock, carnival, corporation, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>ðŸ‘‰</td>\n",
       "      <td>[$SPY, $QQQ]</td>\n",
       "      <td>[Unemployment, Markets, stocks]</td>\n",
       "      <td>[unemployment, market, move, high, mean, stock]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ðŸ‘‰</td>\n",
       "      <td>[$SPY, $SPX]</td>\n",
       "      <td>[recession2020, stocks]</td>\n",
       "      <td>[buyer, slowly, gain, momentum, recession, stock]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label emoji                              Cashtags  \\\n",
       "0      2        [$SPX, $SPY, $ES, $IXIC, $DJI, $QQQ]   \n",
       "1      2  ðŸ‡ºðŸ‡¸ðŸ‡ºðŸ‡¸                   [$SPX, $SPY, $MSFT]   \n",
       "2      0  ðŸ‡ºðŸ‡¸ðŸ‡ºðŸ‡¸              [$SPX, $SPY, $CCL, $WES]   \n",
       "3      2     ðŸ‘‰                          [$SPY, $QQQ]   \n",
       "4      1     ðŸ‘‰                          [$SPY, $SPX]   \n",
       "\n",
       "                      hashtag_list  \\\n",
       "0                [Markets, Stocks]   \n",
       "1                         [stocks]   \n",
       "2                         [stocks]   \n",
       "3  [Unemployment, Markets, stocks]   \n",
       "4          [recession2020, stocks]   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0                                    [market, stock]  \n",
       "1  [earnings, growth, rate, among, large, stock, ...  \n",
       "2  [bad, perform, stock, carnival, corporation, w...  \n",
       "3    [unemployment, market, move, high, mean, stock]  \n",
       "4  [buyer, slowly, gain, momentum, recession, stock]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() # check cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Tweets_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra cleanup for VADER sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vader = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VADER needs a string not a list, so we recompile the list\n",
    "def listCompile(row):\n",
    "    s = ' '\n",
    "    return(s.join(row))\n",
    "\n",
    "df_vader['clean_tweet'] = df_vader['clean_tweet'].apply(lambda x: listCompile(x))\n",
    "df_vader.head()\n",
    "df_vader.to_csv('Tweets_clean_vader.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
